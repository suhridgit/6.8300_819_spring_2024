{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAgQTTYNpA9x"
   },
   "source": [
    "# 6.8300/6.8301 Network Interpretability and Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErA4keUJB2TQ"
   },
   "source": [
    "In this problem set, we will begin by examining how we can visualize the learned features of a pretrained image classifier to better understand how it makes a decision when classifying a particular scene.\n",
    "\n",
    "Then, you'll train your own simple image classifier and work to improve its performance as much as you can.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_wzeAuZpIhG"
   },
   "source": [
    "# Requirements installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-mfzOA1NQvl"
   },
   "source": [
    "First, let's install everything needed to run this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GdEXtjp-QX_",
    "outputId": "65c95600-29fc-412b-e01c-afe6795d0942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (11.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting image\n",
      "  Downloading image-1.5.33.tar.gz (15 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pillow in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from image) (11.1.0)\n",
      "Collecting django (from image)\n",
      "  Obtaining dependency information for django from https://files.pythonhosted.org/packages/ba/0f/7e042df3d462d39ae01b27a09ee76653692442bc3701fbfa6cb38e12889d/Django-5.1.7-py3-none-any.whl.metadata\n",
      "  Downloading Django-5.1.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from image) (1.17.0)\n",
      "Collecting asgiref<4,>=3.8.1 (from django->image)\n",
      "  Obtaining dependency information for asgiref<4,>=3.8.1 from https://files.pythonhosted.org/packages/39/e3/893e8757be2612e6c266d9bb58ad2e3651524b5b40cf56761e985a28b13e/asgiref-3.8.1-py3-none-any.whl.metadata\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting sqlparse>=0.3.1 (from django->image)\n",
      "  Obtaining dependency information for sqlparse>=0.3.1 from https://files.pythonhosted.org/packages/a9/5c/bfd6bd0bf979426d405cc6e71eceb8701b148b16c21d2dc3c261efc61c7b/sqlparse-0.5.3-py3-none-any.whl.metadata\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading Django-5.1.7-py3-none-any.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: image\n",
      "  Building wheel for image (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19524 sha256=7df7f572477e7d1f26c534b5adc6557a409fc816394f436d7130892a5e48c35f\n",
      "  Stored in directory: /Users/suhrid.deshmukh/Library/Caches/pip/wheels/58/30/d8/3212cd83eeeeee0a1f0c7b9b7bd0674a2b9f09342870473a2a\n",
      "Successfully built image\n",
      "Installing collected packages: sqlparse, asgiref, django, image\n",
      "Successfully installed asgiref-3.8.1 django-5.1.7 image-1.5.33 sqlparse-0.5.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: opencv-python in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from opencv-python) (2.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchvision in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: torch==2.6.0 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/suhrid.deshmukh/.pyenv/versions/3.12.0/envs/vision_psets/lib/python3.12/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow\n",
    "!pip install -U image\n",
    "!pip install opencv-python\n",
    "!pip install tqdm\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from IPython.display import Image, clear_output, display\n",
    "\n",
    "# PyTorch will be out main tool for playing with neural networks\n",
    "import torch\n",
    "import torch.hub\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# CPU / GPU\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "# Download function for model\n",
    "import requests\n",
    "def download(url, fn=None):\n",
    "    if fn is None:\n",
    "        fn = url.split('/')[-1]\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        open(fn, 'wb').write(r.content)\n",
    "        print(\"{} downloaded: {:.2f} KB\".format(fn, len(r.content) / 1024.0))\n",
    "    else:\n",
    "        print(\"url not found:\", url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAP97sYrNa-W"
   },
   "source": [
    "We will load PyTorch, our main tool to play with neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHEfVC-to-5V"
   },
   "source": [
    "# Loading Images and PyTorch models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwXeYaYCNisx"
   },
   "source": [
    "Once, we have loaded all the relevant libraries, we will load the model. We will begin with a scene classification model trained on the Places dataset with a ResNet-50 architecture.\n",
    "\n",
    "![texto alternativo](https://www.codeproject.com/KB/AI/1248963/resnet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GxETSgjcC7tb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50_places365.pth.tar downloaded: 94990.39 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=365, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the pretrained weights\n",
    "download('http://6.8300.csail.mit.edu/sp23/psets/pset4/resnet50_places365.pth.tar')\n",
    "\n",
    "# Initialize the model\n",
    "resnet = models.resnet50(num_classes=365)\n",
    "\n",
    "# Load the model\n",
    "sd = torch.load('resnet50_places365.pth.tar', map_location=device)['state_dict']\n",
    "\n",
    "# Don't worry about this -- when you train a model using parallelism, the\n",
    "# weights begin with model. We have 1 cpu/gpu, so we will fix the name keys\n",
    "sd = {k.replace('module.', ''): v for k, v in sd.items()}\n",
    "\n",
    "# Load weights into network\n",
    "resnet.load_state_dict(sd)\n",
    "\n",
    "# Important: put network into evaluation mode\n",
    "# Some networks have layers that do not behave the same during train/eval\n",
    "# Forgetting this is a very common source of bugs\n",
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bF8-ICwh6z7"
   },
   "source": [
    "# Problem 1: Visualizing Network Filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3NL-WaTYN1l"
   },
   "source": [
    "Now, let's write a function to visualize the filters. You have to complete the following code, with one line normalizing the filter values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4S34CUWPkgLW"
   },
   "outputs": [],
   "source": [
    "def showarray(a, fmt='jpeg'):\n",
    "    ''' Helper function. Use this to show your filters\n",
    "\n",
    "    Converting to standard image format is a common task that produces garbage\n",
    "    images when not done correctly. We've provided the correct conversions'''\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    # TODO (1a): Normalize between 0 and 255, format as np.uint8\n",
    "    ans = #TODO\n",
    "    print(ans.min(), ans.max()) #should be 0 and 255\n",
    "    return ans\n",
    "\n",
    "print(resnet.conv1.weight.data.size())  # Print the size of conv1 weights\n",
    "\n",
    "# Display the filters of the initial convolutional layer:\n",
    "for i in range(30):\n",
    "    print('Visualizing conv1 filter', i)\n",
    "    weight = resnet.conv1.weight.data[i, 0, :, :]\n",
    "    # weight = resnet.conv1.weight.data[i, 1, :, :] ## color channels\n",
    "    # weight = resnet.conv1.weight.data[i, 2, :, :] ## color channels\n",
    "    normed = normalize_tensor(weight)\n",
    "\n",
    "    # Resize\n",
    "    normed = cv2.resize(normed, (50, 50))\n",
    "    showarray(normed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iQxKhYP6NLe"
   },
   "source": [
    "## 1b Exercise: Visualize filters for another convolutional layer in ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqZbldo0YjAD"
   },
   "outputs": [],
   "source": [
    "# TODO (1b): pick a different layer to analyze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hykO_Di6iDI6"
   },
   "source": [
    "# Predicting classes with a pre-trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkNZ-26zOQfO"
   },
   "source": [
    "To make the process easier to read, we will load the label <--> index assignment for the Places dataset and one image to use throughout the pset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44Xx19KEH2aV"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
    "classlabels = [\n",
    "    r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')\n",
    "]\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "download('http://6.869.csail.mit.edu/fa19/miniplaces_part1/rio.jpg')\n",
    "img_pil = PIL.Image.open('rio.jpg').convert('RGB')\n",
    "img_numpy = np.array(img_pil)\n",
    "showarray(img_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCwHmgo1pmpF"
   },
   "source": [
    "First, let's take a look at the raw prediction of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCHZcX0lpvzb"
   },
   "outputs": [],
   "source": [
    "# data preprocessing: resize an image, change it from a PIL image to a pytorch tensor, normalize it according to dataset statistics\n",
    "center_crop = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((227, 227)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# our model can only take input that is preprocessed, so preprocess our loaded image\n",
    "img_tensor = center_crop(img_pil)\n",
    "logits = resnet(img_tensor.unsqueeze(0)).squeeze()\n",
    "# get the indices associated with the topk logits\n",
    "categories = logits.topk(5)[1]\n",
    "\n",
    "# print the labels corresponding to the topk indices\n",
    "print(categories)\n",
    "print(', '.join([classlabels[cat] for cat in categories]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znOait62TwB5"
   },
   "source": [
    "# Problem 2: Visualizing Internal Activations of the Network\n",
    "\n",
    "Let's look at what parts of the image cause different units to activate (send some positive signal). All of these activations combine to inform the final inference.\n",
    "\n",
    "The convolutional layers of ResNet essentially make a semantic representation of what is contained in the image. This is followed by two fully connected layers, which use the information from that representation to categorize the image.\n",
    "\n",
    "So, let's remove the last few layers (which do classification) to get the underlying representation, and we'll visualize the activations that went into that representation from different units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drMqLrhbb8_4"
   },
   "outputs": [],
   "source": [
    "def generate_featuremap_unit(model_cut, unit_id, im_input):\n",
    "    # Extract activation from model\n",
    "    # Mark the model as being used for inference\n",
    "    model_cut.eval()\n",
    "    # Crop the image\n",
    "    im = center_crop(im_input)\n",
    "    # Place the image into a batch of size 1, and use the model to get an intermediate representation\n",
    "    activations = model_cut(im.unsqueeze(0))\n",
    "    # Print the shape of our representation\n",
    "    print(activations.size())\n",
    "    # Extract the only result from this batch, and take just the `unit_id`th channel\n",
    "    # Return this channel\n",
    "    return activations.squeeze()[unit_id]\n",
    "\n",
    "def visualize_featuremap(im_input, feature_map, alpha=0.3):\n",
    "    # Normalize to [0..1], with a little leeway (0.9999) in case feature_map has 0 range\n",
    "    feature_map = feature_map / (feature_map.max() + 1e-10)\n",
    "    # Convert to numpy (detach() just seperates a tensor from the gradient)\n",
    "    feat_numpy = feature_map.detach().numpy()\n",
    "    # Resize the feature map to our original image size (our strided conv layers reduce the size of the image)\n",
    "    feat_numpy = cv2.resize(feat_numpy, (im_input.shape[1], im_input.shape[0]))\n",
    "    # Invert to make the heatmap look more natural\n",
    "    map_t = 1 - feat_numpy\n",
    "    # Add an extra dimension to make this a [H,W,C=1] image\n",
    "    feat_numpy = np.expand_dims(feat_numpy, axis=2)\n",
    "\n",
    "    # Convert to image (UINT8 from 0-255)\n",
    "    map_t = 255 * map_t\n",
    "    map_t = map_t.astype(np.uint8)\n",
    "    # Use a color map to change this from BW to a nice color\n",
    "    map_t = cv2.applyColorMap(map_t, cv2.COLORMAP_JET)\n",
    "    # Combine the heatmap with the original image so you can see which section of the image is activated\n",
    "    im_final = np.multiply((alpha * im_input + (1 - alpha) * map_t), feat_numpy) + np.multiply(\n",
    "        im_input, 1 - feat_numpy\n",
    "    )\n",
    "    # Return final visualization\n",
    "    return im_final\n",
    "\n",
    "\n",
    "def remove_last_layers(model, num_layers=2):\n",
    "    # TODO (2a): remove the last 2 layers of resnet\n",
    "    # Note: the .children() function of nn.Module\n",
    "    # (which resnet50 inherits from) and nn.Sequential() will be useful\n",
    "    return model\n",
    "\n",
    "\n",
    "model_cut = remove_last_layers(resnet, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wth2uzqQiOcg"
   },
   "outputs": [],
   "source": [
    "# TODO (2b): search for mountain, sky, and building units by trying out different values of unit_id\n",
    "# Fill in these variables with ints\n",
    "unit_mountain = _\n",
    "unit_sky      = _\n",
    "unit_building = _\n",
    "\n",
    "# Use this to test\n",
    "feat = generate_featuremap_unit(model_cut, YOUR_UNIT_ID_HERE, img_pil)\n",
    "im_final = visualize_featuremap(img_numpy, feat)\n",
    "showarray(im_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh6l1EaXT7JH"
   },
   "outputs": [],
   "source": [
    "# TODO (2c -- required for 6.8300 only) Find the unit index that has the maximum weights\n",
    "# in the fully connected layer and deactivate that unit.\n",
    "# Compare the orginal prediction and the new prediction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out_original = resnet(img_tensor.unsqueeze(0)).squeeze() # original prediction\n",
    "class_ids = np.argsort(-out_original.data.cpu().numpy())[0]\n",
    "index = torch.topk(resnet.fc.weight[class_ids, :], k=5)[1]\n",
    "\n",
    "\n",
    "def deactivate_unit_activations(model_cut, image, unit_id):\n",
    "    # TODO: Get the output features for this model. Feature shape should be (1, # units, H, W)\n",
    "\n",
    "    # TODO: deactivate the unit index that has the maximum weights (Set all values for that unit to 0)\n",
    "\n",
    "    # TODO: run the modified features through the last two layers of the original network\n",
    "\n",
    "    return out_modified\n",
    "\n",
    "\n",
    "def plot_top_classes(values, top_k=5, title=None):\n",
    "    sorted_classes = np.argsort(-values)\n",
    "    class_ids = sorted_classes[:top_k]\n",
    "    class_names = [classlabels[it] for it in list(class_ids)]\n",
    "    class_values = values[class_ids]\n",
    "    print(title + \" top 5 class names \", class_names)\n",
    "    print(title + \" top 5 class values \", class_values)\n",
    "    plt.bar(class_names, class_values)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title(title)\n",
    "\n",
    "# TODO (2d -- required for 6.8300 only): Report the lowest number of dropped\n",
    "# units required to change the top prediction class\n",
    "lowest_dropped_units = _\n",
    "\n",
    "out_modified = deactivate_unit_activations(model_cut, img_tensor, index)\n",
    "\n",
    "plt.figure(0)\n",
    "plot_top_classes(out_original.data.cpu().numpy(), title='Original')\n",
    "plt.figure(1)\n",
    "plot_top_classes(out_modified.data.cpu().numpy(), title='Modified')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j8HRuNSqpmK"
   },
   "source": [
    "# Problem 3: Visualizing model activations with Class Activation Maps (CAMs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFgLrvzcqwzb"
   },
   "source": [
    "Once we have load the image and the model, now we will explore how to visualize the internal activations of the model. We will start by visualizing which parts of the image are responsibe for the final decision.\n",
    "\n",
    "![texto alternativo](https://camo.githubusercontent.com/fb9a2d0813e5d530f49fa074c378cf83959346f7/687474703a2f2f636e6e6c6f63616c697a6174696f6e2e637361696c2e6d69742e6564752f6672616d65776f726b2e6a7067)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6sDff8uOmQl"
   },
   "source": [
    "We create a version of the model without the last two layers, so that we can access the last convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wXXopizPDK-"
   },
   "source": [
    "We compute the activations using the Class Activation Mapping for a given output label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "aTgSTx9QMhOB"
   },
   "outputs": [],
   "source": [
    "def generate_featuremap_CAM(model, unit_id, im_input):\n",
    "    # Extract activation from model\n",
    "    model.eval()\n",
    "    im = center_crop(im_input)\n",
    "    activations = model(im.unsqueeze(0)).squeeze()  # 2048 x h x w\n",
    "    num_channels, height, width = activations.shape\n",
    "\n",
    "    # TODO (3a): convert the shape of the output (activations variable) to (h*w) x c\n",
    "    # Purpose of this question: reshaping (.view) tensors can be tricky. The default\n",
    "    # ordering of dimensions is usually channel first. You may need to .transpose()\n",
    "    # to correct for this. This is one of the most common sources of bugs in\n",
    "    # deep NN implementations\n",
    "    print(activations.shape)\n",
    "\n",
    "    # TODO (3b): Run the fully connected layer from resnet to compute the weighted average with activations as the input variable\n",
    "    # out_final should be a (h*w) x 365 tensor.\n",
    "    print(out_final.shape)\n",
    "\n",
    "    # TODO (3c): obtain the class activation map for the corresponding unit_id\n",
    "    # class_activation_maps should be a 365 x height x width tensor.\n",
    "    class_activation_maps = None\n",
    "    return class_activation_maps[unit_id]\n",
    "\n",
    "# Visualize the most activated region in the image for the 5 top classes\n",
    "for i in range(categories.shape[0]):\n",
    "    print('Visualizing category', classlabels[categories[i]])\n",
    "    feat = generate_featuremap_CAM(model_cut, categories[i].item(), img_pil)\n",
    "    im_result = visualize_featuremap(img_numpy, feat)\n",
    "    showarray(im_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ri-xjKurojv"
   },
   "source": [
    "# Problem 4: Training your own classifier\n",
    "\n",
    "The goal of this problem is to train a small convolutional neural network to classify images of clothing items from the FashionMNIST dataset. You'll first fill in critical components of a simple PyTorch training pipeline, evaluate the model on the test set, and explore the impact of specific design choices and hyperparameters on the model's performance.\n",
    "\n",
    "## Building an FashionMNIST Classifier\n",
    "\n",
    "Using what we have learned, let's build a simple FasionMNIST classifier. Each element of the dataset is a 2-tuple: the 28x28 image and its label.\n",
    "\n",
    "\n",
    "The [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html) library provides a wide range of standard vision datasets and networks with pretrained weights. We will use [the `torchvision.datasets.FashionMNIST` class](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST) to easily access the FasionMNIST dataset.\n",
    "\n",
    "In deep learning, it is often a good idea to normalize network inputs to be centered around zero. We use the [`torchvision.transforms.Normalize`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize) tranform to achieve this. We compose the transforms together using the [`torchvision.transforms.Compose`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose) transform, which allows us to apply multiple transforms sequentially. We provide the following functions to help create the FashionMNIST dataset with these transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_1gMf_dMWgp"
   },
   "outputs": [],
   "source": [
    "def get_transform(split='train'):\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def get_dataset(split='train', transform=None):\n",
    "    return datasets.FashionMNIST(\n",
    "        'data',\n",
    "        train=split == 'train',\n",
    "        download=True,\n",
    "        transform=transform if transform is not None else transforms.ToTensor(),\n",
    "    )\n",
    "\n",
    "def get_dataloaders(batch_size=64, num_workers=2):\n",
    "    '''Use Pytorch torch.utils.data.DataLoader to load batched data'''\n",
    "    train_dataset = get_dataset('train', get_transform())\n",
    "    val_dataset = get_dataset('val', get_transform())\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,  # shuffle training set\n",
    "        num_workers=num_workers,  # turns on multi-processing loading so training is not blocked by data loading\n",
    "        pin_memory=True,  # pin_memory allows faster transfer from CPU to GPU\n",
    "    )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "print('training set size:\\t{}'.format(len(get_dataset('train'))))\n",
    "print('validation set size:\\t{}'.format(len(get_dataset('val'))))\n",
    "\n",
    "# Each element yielded by `train_loader` (a Python iterable) is still a 2-tuple,\n",
    "# but now consisting of a batched image tensor, and a batched label tensor.\n",
    "train_loader, _ = get_dataloaders()\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print('batched image tensor shape: {}'.format(images.shape))\n",
    "print('batched label tensor shape: {}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYanihczMWgq"
   },
   "source": [
    "### Building the Network\n",
    "\n",
    "We will use a convolutional network for classification. The following architecture is adapted from the famous [LeNet-5](https://ieeexplore.ieee.org/document/726791) [1].\n",
    "\n",
    "[1] LeCun, Yann, et al. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86.11 (1998): 2278-2324.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjTkfOzJMWgq"
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        kernel_size=5,\n",
    "        filter1_size=5,\n",
    "        filter2_size=16,\n",
    "        fc1_size=120,\n",
    "        fc2_size=84,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=filter1_size,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=filter1_size,\n",
    "            out_channels=filter2_size,\n",
    "            kernel_size=kernel_size,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=filter2_size * kernel_size ** 2, out_features=fc1_size)\n",
    "        self.fc2 = nn.Linear(in_features=fc1_size, out_features=fc2_size)\n",
    "        self.fc3 = nn.Linear(in_features=fc2_size, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "net = MyNet().to(device)\n",
    "\n",
    "# This network output a size 10 vector for each input image, as verified below\n",
    "# using a random input tensor.\n",
    "net(torch.randn(32, 1, 28, 28, device=device)).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnEKfIXtMWgr"
   },
   "source": [
    "### **Problem 4a: Training Loop**\n",
    "\n",
    "For classification, we will use the cross-entropy loss [`F.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html?highlight=cross_entropy#torch.nn.functional.cross_entropy) to train this network.\n",
    "\n",
    "We write a function that accepts a PyTorch dataloader, model and optimizer (as well as a device and current epoch), and trains the net for 1 epoch (one full pass through the training set).\n",
    "\n",
    "The next exercise is to fill in the code below. You can use the following pytorch functions:\n",
    "\n",
    "-   put data on GPU: [to](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html?highlight=#torch.to)\n",
    "-   clear gradient: [zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html?highlight=zero_grad)\n",
    "-   backward pass: [backward](https://pytorch.org/docs/stable/generated/torch.autograd.backward.html?highlight=backward#torch.autograd.backward)\n",
    "-   update parameters with a gradient step: [step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html?highlight=step#torch.optim.Optimizer.step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZDujtWfMWgs"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#                      #\n",
    "#       Exercise       #\n",
    "#                      #\n",
    "########################\n",
    "\n",
    "# Fix the places with a TODO\n",
    "\n",
    "def accuracy(output, target):\n",
    "    \"\"\"Computes the accuracy of the model on the test set.\n",
    "\n",
    "    Args:\n",
    "        output: the output of the model\n",
    "        target: the ground truth labels\n",
    "    Returns:\n",
    "        the accuracy of the model on the test set\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct = (predicted == target).sum().item()\n",
    "    return correct / len(target)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train(dataloader, model, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    acc_meter = AverageMeter()\n",
    "    batches = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    batches.set_description(\"Epoch NA: Loss (NA) Accuracy (NA %)\")\n",
    "    for batch_idx, (data, target) in batches:\n",
    "\n",
    "        # TODO: Move data to appropriate device\n",
    "\n",
    "        # TODO: Zero out gradients\n",
    "\n",
    "        # TODO: Compute forward pass, loss, and gradients\n",
    "\n",
    "        # TODO: Update parameters\n",
    "\n",
    "        # TODO: Compute and record accuracy (cf. helper function above)\n",
    "        acc = None\n",
    "        acc_meter.update(acc)\n",
    "\n",
    "        batches.set_description(\n",
    "            \"Epoch {:d}: Loss ({:.2e}), Train Accuracy ({:02.0f}%)\".format(\n",
    "                epoch, loss.item(), 100.0 * acc_meter.avg\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return train_loss, acc_meter.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyNf_5FdMWgt"
   },
   "source": [
    "## Problem 4b: Evaluating the Model\n",
    "\n",
    "Let's also write a function that evaluates our network on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKvDjGqdMWgu"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#                      #\n",
    "#       Exercise       #\n",
    "#                      #\n",
    "########################\n",
    "\n",
    "# Fix the places with a TODO\n",
    "\n",
    "\n",
    "def get_prediction(image, net):\n",
    "    # TODO!!!  (HINT: use .argmax(dim=-1))\n",
    "    #   `prediction` should be an integer vector of size equal to the batch size.\n",
    "    #   Remember that the network outputs logits of the prediction probabilities,\n",
    "    #   and that the higher the logits, the higher the probability.\n",
    "    return\n",
    "\n",
    "def evaluate(dataloader, net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    net.eval()  # puts the network in eval mode. this is important when the\n",
    "    # network has layers that behaves differently in training and\n",
    "    # evaluation time, e.g., dropout and batch norm.\n",
    "    for image, label in dataloader:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():  # gradients are not tracked in this context manager\n",
    "            # since we are evaluating, gradients are not needed\n",
    "            # and we can save some time and GPU memory.\n",
    "\n",
    "            # TODO: Get predictions using function get_prediction\n",
    "\n",
    "            # TODO: Update total and correct\n",
    "\n",
    "            # Hint: Your logic can reflect what's already implemented for you above\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMTf__RUMWgu"
   },
   "outputs": [],
   "source": [
    "# Without any training, the network accuracy matches that of random guessing: ~10%.\n",
    "_, val_loader = get_dataloaders(batch_size=32, num_workers=0)\n",
    "print('At initialization, the network has accuracy {:.4f}%'.format(evaluate(val_loader, net) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D4baaO8MWgu"
   },
   "source": [
    "## Problem 4c: Putting Everything Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1ZpNabEMWgv"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#                      #\n",
    "#       Exercise       #\n",
    "#                      #\n",
    "########################\n",
    "\n",
    "\n",
    "# Fix the places with a TODO\n",
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "def create_optimizer(net, lr):\n",
    "    # TODO: Create optimizer\n",
    "    return\n",
    "\n",
    "optim = create_optimizer(net, lr)\n",
    "train_loader, val_loader = get_dataloaders(batch_size=128)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch: {}\\tValidation Accuracy: {:.4f}%'.format(epoch, evaluate(val_loader, net) * 100))\n",
    "    train(train_loader, net, optim, epoch)\n",
    "\n",
    "valid_accuracy = evaluate(val_loader, net) * 100\n",
    "print('Done! \\tValidation Accuracy: {:.4f}%'.format(valid_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTdHjIz3MWgv"
   },
   "source": [
    "## Problem 4d (required for 6.8300 only)\n",
    "\n",
    "\n",
    "\n",
    "We want you to get a feel for the impact of specific design choices on the performance of the network. Experiment with two or more of the following hyperparameters / techniques:\n",
    "\n",
    "-   Data augmentation\n",
    "-   Weight initialization\n",
    "-   Number of layers, or number of layer features\n",
    "-   Type of optimizer\n",
    "-   Learning rate and/or schedule\n",
    "-   Regularization\n",
    "\n",
    "For the techniques you choose, plot the top-1 accuracy of your modified network against the top-1 accuracy of the original network for both the training and validation sets. Try several different hyperparameter values! For example, if you choose to modify the learning rate, you can plot a chart of learning rate vs. top-1 accuracy. Briefly describe the techniques you tried, and suggest an explanation for your results. **Full credit will only be given if at least 90% validation accuracy is achieved (i.e., the accuracy must be above 89.9999999%).**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "vision_psets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
