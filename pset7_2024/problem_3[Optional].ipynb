{"cells":[{"cell_type":"markdown","metadata":{"id":"qEsNHTtVlbkV"},"source":["## Problem 3: Making our own Dreambooth model for Stable Diffusion!\n","\n","In this problem, we'll be making a dreambooth model for Stable Diffusion. We'll walk you through loading your images, preparing them for the model, and ultimately training a dreambooth model with optimized settings.\n","\n","## What is Dreambooth?\n","Dreambooth is a method to introduce a new concept, like your own face, your dog or a specific artstyle into text-to-image diffusion models. It works by carefully finetuning a pretrained diffusion model with a few images from the target concept (e.g. your pet dobermann) while preserving variety in the target class (e.g. class \"dog\") with images generated by the model itself.\n","\n","With 5-20 images from the target concept and a target class to overload (e.g. \"man\", \"dog\", etc), you can create a checkpoint that \"understands\" your concept and can generate it in different poses, styles and scenes by prompting it with a specific keyword (defined during training).\n","\n","Check the [Project Page](https://dreambooth.github.io/), a nice [open-source implementation](https://github.com/XavierXiao/Dreambooth-Stable-Diffusion), and this [blogpost](https://stable-diffusion-art.com/dreambooth/) if you'd like to know more.\n","\n","![dreambooth](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)\n","\n","## What you need before starting\n","- A folder with more than 5 images for your specific concept. Each image should be named \"yourkeyword (1).jpg\", \"yourkeyword (2).jpg\", etc\n","- Google Colab with a GPU or a local GPU with at least 15gb VRAM\n","\n","## Your objective for this problem: compare any two dreambooth models and report results.\n","\n","We'd like you to try 2 dreambooth models and compare the results. You have complete freedom to test any two dreambooth models, including:\n","- Checkpoint 1 trained for X steps vs Checkpoint 2 resumed from Checkpoint 1 and trained for another Y steps\n","- Checkpoint 1 with an object as concept vs Checkpoint 2 trained with a style as concept\n","- Checkpoint 1 with one set of training parameters vs Checkpoint 2 trained with another set\n","- Any other meaningful difference that you'd like to explore!\n","\n","\n","*Code and Notebook adapted by Camilo Fosco from fast-DreamBooth by TheLastBen at https://github.com/TheLastBen/fast-stable-diffusion*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4Bae3VP6UsE"},"outputs":[],"source":["# Let's connect our google drive to save checkpoints and images.\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyvcqeiL65Tj"},"outputs":[],"source":["#@markdown # Dependencies\n","\n","#@markdown Imports, install a few libraries and download code from the necessary githubs\n","\n","\n","from IPython.utils import capture\n","import time\n","import os\n","\n","print('\u001b[1;32mInstalling dependencies...')\n","with capture.capture_output() as cap:\n","    %cd /content/\n","    !pip install -qq --no-deps accelerate==0.12.0\n","    !wget -q -i https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dependencies/dbdeps.txt\n","    !dpkg -i *.deb\n","    !tar -C / --zstd -xf gcolabdeps.tar.zst\n","    !rm *.deb | rm *.zst | rm *.txt\n","    !git clone -q --depth 1 --branch main https://github.com/TheLastBen/diffusers\n","    !pip install gradio==3.16.2 --no-deps -qq\n","    %env LD_PRELOAD=libtcmalloc.so\n","    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","    os.environ['PYTHONWARNINGS'] = 'ignore'\n","\n","print('\u001b[1;32mDone, proceed')"]},{"cell_type":"markdown","metadata":{"id":"R3SsbIlxw66N"},"source":["# Model Download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3KHGKqyeJp9"},"outputs":[],"source":["import os\n","import time\n","from IPython.utils import capture\n","from IPython.display import clear_output\n","import wget\n","from subprocess import check_output\n","import urllib.request\n","import base64\n","\n","#@markdown Choose the stable diffusion version you want to finetune: 1.5 is artistic and varied, 2.1-512px is more realistic but usually less varied and more finicky with prompts, and 2.1-768px is similar to 512 but supports native 768px resolution.\n","\n","Model_Version = \"1.5\" #@param [ \"1.5\", \"V2.1-512px\", \"V2.1-768px\"]\n","\n","\n","with capture.capture_output() as cap:\n","  %cd /content/\n","\n","\n","# Uncomment if you'd like to set your own model path or ckpt, including `safetensors` checkpoints - not well tested\n","\n","# MODEL_PATH = \"\" #@param {type:\"string\"}\n","\n","# MODEL_LINK = \"\" #@param {type:\"string\"}\n","\n","# safetensors = False #@param {type:\"boolean\"}\n","\n","# sftnsr=\"\"\n","# if not safetensors:\n","#   modelnm=\"model.ckpt\"\n","# else:\n","#   modelnm=\"model.safetensors\"\n","#   sftnsr=\"--from_safetensors\"\n","\n","# if os.path.exists('/content/gdrive/MyDrive/Fast-Dreambooth/token.txt'):\n","#   with open(\"/content/gdrive/MyDrive/Fast-Dreambooth/token.txt\") as f:\n","#      token = f.read()\n","#   authe=f'https://USER:{token}@'\n","# else:\n","#   authe=\"https://\"\n","\n","def downloadmodel():\n","\n","  if os.path.exists('/content/stable-diffusion-v1-5'):\n","    !rm -r /content/stable-diffusion-v1-5\n","  clear_output()\n","\n","  %cd /content/\n","  clear_output()\n","  !mkdir /content/stable-diffusion-v1-5\n","  %cd /content/stable-diffusion-v1-5\n","  !git init\n","  !git lfs install --system --skip-repo\n","  !git remote add -f origin  \"https://huggingface.co/runwayml/stable-diffusion-v1-5\"\n","  !git config core.sparsecheckout true\n","  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\\n!vae/diffusion_pytorch_model.bin\\n!*.safetensors\" > .git/info/sparse-checkout\n","  !git pull origin main\n","  if os.path.exists('/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n","    !wget -q -O vae/diffusion_pytorch_model.bin https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.bin\n","    !rm -r .git\n","    !rm model_index.json\n","    time.sleep(1)\n","    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n","    %cd /content/\n","    clear_output()\n","    print('\u001b[1;32mDONE !')\n","  else:\n","    while not os.path.exists('/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n","         print('\u001b[1;31mSomething went wrong')\n","         time.sleep(5)\n","\n","def newdownloadmodel():\n","\n","  %cd /content/\n","  clear_output()\n","  !mkdir /content/stable-diffusion-v2-768\n","  %cd /content/stable-diffusion-v2-768\n","  !git init\n","  !git lfs install --system --skip-repo\n","  !git remote add -f origin  \"https://huggingface.co/stabilityai/stable-diffusion-2-1\"\n","  !git config core.sparsecheckout true\n","  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\\n!*.safetensors\" > .git/info/sparse-checkout\n","  !git pull origin main\n","  !rm -r /content/stable-diffusion-v2-768/.git\n","  %cd /content/\n","  clear_output()\n","  print('\u001b[1;32mDONE !')\n","\n","\n","def newdownloadmodelb():\n","\n","  %cd /content/\n","  clear_output()\n","  !mkdir /content/stable-diffusion-v2-512\n","  %cd /content/stable-diffusion-v2-512\n","  !git init\n","  !git lfs install --system --skip-repo\n","  !git remote add -f origin  \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base\"\n","  !git config core.sparsecheckout true\n","  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\\n!*.safetensors\" > .git/info/sparse-checkout\n","  !git pull origin main\n","  !rm -r /content/stable-diffusion-v2-512/.git\n","  %cd /content/\n","  clear_output()\n","  print('\u001b[1;32mDONE !')\n","\n","if Model_Version==\"1.5\":\n","  if not os.path.exists('/content/stable-diffusion-v1-5'):\n","    downloadmodel()\n","    MODEL_NAME=\"/content/stable-diffusion-v1-5\"\n","  else:\n","    MODEL_NAME=\"/content/stable-diffusion-v1-5\"\n","    print(\"\u001b[1;32mThe v1.5 model already exists, using this model.\")\n","elif Model_Version==\"V2.1-512px\":\n","  if not os.path.exists('/content/stable-diffusion-v2-512'):\n","    newdownloadmodelb()\n","    MODEL_NAME=\"/content/stable-diffusion-v2-512\"\n","  else:\n","    MODEL_NAME=\"/content/stable-diffusion-v2-512\"\n","    print(\"\u001b[1;32mThe v2-512px model already exists, using this model.\")\n","elif Model_Version==\"V2.1-768px\":\n","  if not os.path.exists('/content/stable-diffusion-v2-768'):\n","    newdownloadmodel()\n","    MODEL_NAME=\"/content/stable-diffusion-v2-768\"\n","  else:\n","    MODEL_NAME=\"/content/stable-diffusion-v2-768\"\n","    print(\"\u001b[1;32mThe v2-768px model already exists, using this model.\")"]},{"cell_type":"markdown","metadata":{"id":"0tN76Cj5P3RL"},"source":["# Dreambooth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1B299g-_VJo"},"outputs":[],"source":["import os\n","from IPython.display import clear_output\n","from IPython.utils import capture\n","from os import listdir\n","from os.path import isfile\n","from subprocess import check_output\n","import wget\n","import time\n","\n","#@markdown #Create/Load a Session\n","#@markdown To avoid retraining if you have to interrupt this exercise, we'll create a Session that you can later reload. Enter a name for you session below.\n","\n","#@markdown Your sessions will be stored in your gdrive under `\"AdvancesInComputerVision_Pset7_Dreambooth/Sessions\"`\n","\n","\n","try:\n","  MODEL_NAME\n","  pass\n","except:\n","  MODEL_NAME=\"\"\n","\n","PT=\"\"\n","\n","Session_Name = \"\" #@param{type: 'string'}\n","while Session_Name==\"\":\n","  print('\u001b[1;31mInput the Session Name:')\n","  Session_Name=input('')\n","Session_Name=Session_Name.replace(\" \",\"_\")\n","\n","#@markdown - Enter the session name, it if it exists, it will load it, otherwise it'll create a new session.\n","\n","# Session_Link_optional = \"\" #@param{type: 'string'}\n","\n","# #@markdown - Import a session from another gdrive, the shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove any intermediary CKPT if any.\n","\n","WORKSPACE='/content/gdrive/MyDrive/AdvancesInComputerVision_Pset7_Dreambooth'\n","\n","\n","INSTANCE_NAME=Session_Name\n","OUTPUT_DIR=\"/content/models/\"+Session_Name\n","SESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\n","INSTANCE_DIR=SESSION_DIR+'/instance_images'\n","CONCEPT_DIR=SESSION_DIR+'/concept_images'\n","CAPTIONS_DIR=SESSION_DIR+'/captions'\n","MDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\n","\n","if os.path.exists(str(SESSION_DIR)):\n","  mdls=[ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]\n","  if not os.path.exists(MDLPTH) and '.ckpt' in str(mdls):\n","\n","    def f(n):\n","      k=0\n","      for i in mdls:\n","        if k==n:\n","          !mv \"$SESSION_DIR/$i\" $MDLPTH\n","        k=k+1\n","\n","    k=0\n","    print('\u001b[1;33mNo final checkpoint model found, select which intermediary checkpoint to use, enter only the number, (000 to skip):\\n\u001b[1;34m')\n","\n","    for i in mdls:\n","      print(str(k)+'- '+i)\n","      k=k+1\n","    n=input()\n","    while int(n)>k-1:\n","      n=input()\n","    if n!=\"000\":\n","      f(int(n))\n","      print('\u001b[1;32mUsing the model '+ mdls[int(n)]+\" ...\")\n","      time.sleep(2)\n","    else:\n","      print('\u001b[1;32mSkipping the intermediary checkpoints.')\n","    del n\n","\n","with capture.capture_output() as cap:\n","  %cd /content\n","  resume=False\n","\n","if os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n","  print('\u001b[1;32mLoading session with no previous model, using the original model or the custom downloaded model')\n","  if MODEL_NAME==\"\":\n","    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n","  else:\n","    print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n","\n","elif os.path.exists(MDLPTH):\n","  print('\u001b[1;32mSession found, loading the trained model ...')\n","  wget.download('https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/det.py')\n","  print('\u001b[1;33mDetecting model version...')\n","  Model_Version=check_output('python det.py --MODEL_PATH '+MDLPTH, shell=True).decode('utf-8').replace('\\n', '')\n","  clear_output()\n","  print('\u001b[1;32m'+Model_Version+' Detected')\n","  !rm det.py\n","  if Model_Version=='1.5':\n","    !wget -q -O config.yaml https://github.com/CompVis/stable-diffusion/raw/main/configs/stable-diffusion/v1-inference.yaml\n","    print('\u001b[1;32mSession found, loading the trained model ...')\n","    !python /content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path $MDLPTH --dump_path \"$OUTPUT_DIR\" --original_config_file config.yaml\n","    !rm /content/config.yaml\n","\n","  elif Model_Version=='V2.1-512px':\n","    !wget -q -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n","    print('\u001b[1;32mSession found, loading the trained model ...')\n","    !python /content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v2 --reference_model stabilityai/stable-diffusion-2-1-base\n","    !rm /content/convertodiff.py\n","\n","  elif Model_Version=='V2.1-768px':\n","    !wget -q -O convertodiff.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertodiffv2-768.py\n","    print('\u001b[1;32mSession found, loading the trained model ...')\n","    !python /content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v2 --reference_model stabilityai/stable-diffusion-2-1\n","    !rm /content/convertodiff.py\n","\n","\n","  if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","    resume=True\n","    clear_output()\n","    print('\u001b[1;32mSession loaded.')\n","  else:\n","    if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","      print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n","\n","elif not os.path.exists(str(SESSION_DIR)):\n","    %mkdir -p \"$INSTANCE_DIR\"\n","    print('\u001b[1;32mCreating session...')\n","    if MODEL_NAME==\"\":\n","      print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n","    else:\n","      print('\u001b[1;32mSession created, proceed to upload instance images')\n","\n","    #@markdown\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LC4ukG60fgMy"},"outputs":[],"source":["import shutil\n","from google.colab import files\n","import time\n","from PIL import Image\n","from tqdm import tqdm\n","import ipywidgets as widgets\n","from io import BytesIO\n","import wget\n","\n","with capture.capture_output() as cap:\n","  %cd /content\n","  if not os.path.exists(\"/content/smart_crop.py\"):\n","    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/smart_crop.py')\n","  from smart_crop import *\n","\n","#@markdown # Upload Images for your concept\n","#@markdown Run this cell to upload your concept's images. A button will appear below this cell. Click it and you'll be able to multi-select a set of images from a folder in your computer. Select all images in one go!\n","#@markdown ### IMPORTANT: rename the instance pictures of each subject to a unique unknown identifier:\n","#@markdown  - If you have 10 pictures of yourself, rename them all to a chosen identifier with a number in parenthesis after it. For example, if your chosen identifier is `phtmejhn`, the files would be : `phtmejhn (1).jpg`, `phtmejhn (2).png`, etc.\n","#@markdown  Here is an example of what your folder should look like: https://i.imgur.com/d2lD3rz.jpeg\n","\n","\n","#@markdown # Parameters\n","#@markdown No need to modify any of these if running for the first time.\n","\n","#@markdown   **Remove_existing_instance_images:** if checked, will delete the images in your concepts folder before asking for a new upload. Uncheck the box to keep the existing concept images.\n","Remove_existing_instance_images= True #@param{type: 'boolean'}\n","\n","if Remove_existing_instance_images:\n","  if os.path.exists(str(INSTANCE_DIR)):\n","    !rm -r \"$INSTANCE_DIR\"\n","  if os.path.exists(str(CAPTIONS_DIR)):\n","    !rm -r \"$CAPTIONS_DIR\"\n","\n","if not os.path.exists(str(INSTANCE_DIR)):\n","  %mkdir -p \"$INSTANCE_DIR\"\n","if not os.path.exists(str(CAPTIONS_DIR)):\n","  %mkdir -p \"$CAPTIONS_DIR\"\n","\n","if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n","  %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n","\n","\n","#@markdown **IMAGES_FOLDER_OPTIONAL:** If you prefer to specify an existing folder in your gdrive instead of uploading your concept images, point to the folder below. Images in this folder will be added to the existing (if any) concept images. Leave EMPTY to upload images manually.\n","IMAGES_FOLDER_OPTIONAL=\"\" #@param{type: 'string'}\n","\n","#@markdown **Smart_Crop_images and Crop_size:** If checked, images will be smart cropped attempting to keep the main subject in center frame. You can select the size of the square crop with Crop_size. Leave the defaults if in doubt.\n","Smart_Crop_images= True #@param{type: 'boolean'}\n","Crop_size = 512 #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"] {type:\"raw\"}\n","\n","while IMAGES_FOLDER_OPTIONAL !=\"\" and not os.path.exists(str(IMAGES_FOLDER_OPTIONAL)):\n","  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n","  IMAGES_FOLDER_OPTIONAL=input('')\n","\n","if IMAGES_FOLDER_OPTIONAL!=\"\":\n","  if os.path.exists(IMAGES_FOLDER_OPTIONAL+\"/.ipynb_checkpoints\"):\n","    %rm -r \"$IMAGES_FOLDER_OPTIONAL\"\"/.ipynb_checkpoints\"\n","\n","  with capture.capture_output() as cap:\n","    !mv $IMAGES_FOLDER_OPTIONAL/*.txt $CAPTIONS_DIR\n","  if Smart_Crop_images:\n","    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","      extension = filename.split(\".\")[-1]\n","      identifier=filename.split(\".\")[0]\n","      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n","      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n","      width, height = file.size\n","      if file.size !=(Crop_size, Crop_size):\n","        image=crop_image(file, Crop_size)\n","        if extension.upper()==\"JPG\" or extension.upper()==\"jpg\":\n","            image[0] = image[0].convert(\"RGB\")\n","            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n","        else:\n","            image[0].save(new_path_with_file, format=extension.upper())\n","      else:\n","        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n","\n","  else:\n","    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n","\n","  print('\\n\u001b[1;32mDone, proceed to the next cell')\n","\n","\n","elif IMAGES_FOLDER_OPTIONAL ==\"\":\n","  up=\"\"\n","  uploaded = files.upload()\n","  for filename in uploaded.keys():\n","    if filename.split(\".\")[-1]==\"txt\":\n","      shutil.move(filename, CAPTIONS_DIR)\n","    up=[filename for filename in uploaded.keys() if filename.split(\".\")[-1]!=\"txt\"]\n","  if Smart_Crop_images:\n","    for filename in tqdm(up, bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","      shutil.move(filename, INSTANCE_DIR)\n","      extension = filename.split(\".\")[-1]\n","      identifier=filename.split(\".\")[0]\n","      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n","      file = Image.open(new_path_with_file)\n","      width, height = file.size\n","      if file.size !=(Crop_size, Crop_size):\n","        image=crop_image(file, Crop_size)\n","        if extension.upper()==\"JPG\" or extension.upper()==\"jpg\":\n","            image[0] = image[0].convert(\"RGB\")\n","            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n","        else:\n","            image[0].save(new_path_with_file, format=extension.upper())\n","      clear_output()\n","  else:\n","    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","      shutil.move(filename, INSTANCE_DIR)\n","      clear_output()\n","  print('\\n\u001b[1;32mDone, proceed to the next cell')\n","\n","with capture.capture_output() as cap:\n","  %cd \"$INSTANCE_DIR\"\n","  !find . -name \"* *\" -type f | rename 's/ /-/g'\n","  %cd \"$CAPTIONS_DIR\"\n","  !find . -name \"* *\" -type f | rename 's/ /-/g'\n","\n","  %cd $SESSION_DIR\n","  !rm instance_images.zip captions.zip\n","  !zip -r instance_images instance_images\n","  !zip -r captions captions\n","  %cd /content"]},{"cell_type":"markdown","metadata":{"id":"ZnmQYfZilzY6"},"source":["# Training\n","This will take around 15-20 minutes if running on Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-9QbkfAVYYU"},"outputs":[],"source":["\n","#@markdown #Train DreamBooth!\n","#@markdown ---\n","import os\n","from IPython.display import clear_output\n","from google.colab import runtime\n","from subprocess import getoutput\n","import time\n","import random\n","\n","if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n","  %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n","\n","if os.path.exists(CONCEPT_DIR+\"/.ipynb_checkpoints\"):\n","  %rm -r $CONCEPT_DIR\"/.ipynb_checkpoints\"\n","\n","if os.path.exists(CAPTIONS_DIR+\"/.ipynb_checkpoints\"):\n","  %rm -r $CAPTIONS_DIR\"/.ipynb_checkpoints\"\n","\n","\n","#@markdown **Resume_Training:** Check if you'd like to continue training a model that's already been trained. If you test your model with the cell below and you're not satisfied with the result, you can run this cell again with the box checked to continue training for another `UNet_Training_Steps` steps.\n","Resume_Training = False #@param {type:\"boolean\"}\n","\n","if resume and not Resume_Training:\n","  print('\u001b[1;31mOverwrite your previously trained model ? answering \"yes\" will train a new model, answering \"no\" will resume the training of the previous model?  yes or no ?\u001b[0m')\n","  while True:\n","    ansres=input('')\n","    if ansres=='no':\n","      Resume_Training = True\n","      break\n","    elif ansres=='yes':\n","      Resume_Training = False\n","      resume= False\n","      break\n","\n","while not Resume_Training and MODEL_NAME==\"\":\n","  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n","  time.sleep(5)\n","\n","MODELT_NAME=MODEL_NAME\n","\n","#@markdown #Training parameters\n","\n","UNet_Training_Steps=1500 #@param{type: 'number'}\n","UNet_Learning_Rate = 2e-6 #@param [\"2e-5\",\"1e-5\",\"9e-6\",\"8e-6\",\"7e-6\",\"6e-6\",\"5e-6\", \"4e-6\", \"3e-6\", \"2e-6\"] {type:\"raw\"}\n","untlr=UNet_Learning_Rate\n","\n","#@markdown - These default settings are for a dataset of 10 pictures which is enough for training a face. Start with 1500 or lower, test the model, if not enough, resume training for 200 steps, keep testing until you get the desired output. You can also set the UNet_Training_Steps to 0 to train only the text_encoder.\n","\n","Text_Encoder_Training_Steps=350 #@param{type: 'number'}\n","\n","#@markdown - 200-450 steps is enough for a small dataset, keep this number small to avoid overfitting, set to 0 to disable. If starting from a trained checkpoint, setting this to 0 is advisable to avoid overfitting.\n","\n","Text_Encoder_Learning_Rate = 1e-6 #@param [\"2e-6\", \"1e-6\",\"8e-7\",\"6e-7\",\"5e-7\",\"4e-7\"] {type:\"raw\"}\n","txlr=Text_Encoder_Learning_Rate\n","\n","#@markdown - Learning rate for both text_encoder and concept_text_encoder, keep it low to avoid overfitting (1e-6 is higher than 4e-7)\n","\n","Text_Encoder_Concept_Training_Steps=0 #@param{type: 'number'}\n","\n","#@markdown - Suitable for training a style/concept as it acts as heavy regularization, set it to 1500 steps for 200 concept images (you can go higher), set to 0 to disable, set both the settings above to 0 to fintune only the text_encoder on the concept, `set it to 0 before resuming training if it is already trained`.\n","\n","trnonltxt=\"\"\n","if UNet_Training_Steps==0:\n","   trnonltxt=\"--train_only_text_encoder\"\n","\n","Seed=''\n","\n","ofstnse=\"\"\n","Offset_Noise = False #@param {type:\"boolean\"}\n","#@markdown - This allows the model to vary the mean of the starting noise. Offset_Noise is Highly recommended if you're training a style.\n","\n","if Offset_Noise:\n","  ofstnse=\"--offset_noise\"\n","\n","# External_Captions = False #@param {type:\"boolean\"}\n","# #@markdown - Get the captions from a text file for each instance image.\n","extrnlcptn=\"\"\n","# if External_Captions:\n","#   extrnlcptn=\"--external_captions\"\n","\n","Resolution = \"512\" # param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\n","Res=int(Resolution)\n","\n","fp16 = True\n","\n","if Seed =='' or Seed=='0':\n","  Seed=random.randint(1, 999999)\n","else:\n","  Seed=int(Seed)\n","\n","if fp16:\n","  prec=\"fp16\"\n","else:\n","  prec=\"no\"\n","\n","precision=prec\n","\n","resuming=\"\"\n","if Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","  MODELT_NAME=OUTPUT_DIR\n","  print('\u001b[1;32mResuming Training...\u001b[0m')\n","  resuming=\"Yes\"\n","elif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m')\n","  MODELT_NAME=MODEL_NAME\n","  while MODEL_NAME==\"\":\n","    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n","    time.sleep(5)\n","\n","V2=False\n","if os.path.getsize(MODELT_NAME+\"/text_encoder/pytorch_model.bin\") > 670901463:\n","  V2=True\n","\n","s = getoutput('nvidia-smi')\n","GCUNET=\"--gradient_checkpointing\"\n","TexRes=Res\n","if Res<=768:\n","  GCUNET=\"\"\n","\n","if V2:\n","  if Res>704:\n","    GCUNET=\"--gradient_checkpointing\"\n","  if Res>576:\n","    TexRes=576\n","\n","if 'A100' in s :\n","   GCUNET=\"\"\n","   TexRes=Res\n","\n","\n","Enable_text_encoder_training= True\n","Enable_Text_Encoder_Concept_Training= True\n","\n","if Text_Encoder_Training_Steps==0 :\n","   Enable_text_encoder_training= False\n","else:\n","  stptxt=Text_Encoder_Training_Steps\n","\n","if Text_Encoder_Concept_Training_Steps==0:\n","   Enable_Text_Encoder_Concept_Training= False\n","else:\n","  stptxtc=Text_Encoder_Concept_Training_Steps\n","\n","#@markdown ---------------------------\n","Save_Checkpoint_Every_n_Steps = False #@param {type:\"boolean\"}\n","Save_Checkpoint_Every=500 #@param{type: 'number'}\n","if Save_Checkpoint_Every==None:\n","  Save_Checkpoint_Every=1\n","#@markdown - Minimum 200 steps between each save.\n","stp=0\n","Start_saving_from_the_step=500 #@param{type: 'number'}\n","if Start_saving_from_the_step==None:\n","  Start_saving_from_the_step=0\n","if (Start_saving_from_the_step < 200):\n","  Start_saving_from_the_step=Save_Checkpoint_Every\n","stpsv=Start_saving_from_the_step\n","if Save_Checkpoint_Every_n_Steps:\n","  stp=Save_Checkpoint_Every\n","#@markdown - Start saving intermediary checkpoints from this step.\n","\n","Disconnect_after_training=False #@param {type:\"boolean\"}\n","\n","#@markdown - Auto-disconnect from google colab after the training to avoid wasting compute units.\n","\n","def dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps):\n","\n","    !accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py \\\n","    $trnonltxt \\\n","    $extrnlcptn \\\n","    $ofstnse \\\n","    --image_captions_filename \\\n","    --train_text_encoder \\\n","    --dump_only_text_encoder \\\n","    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n","    --instance_data_dir=\"$INSTANCE_DIR\" \\\n","    --output_dir=\"$OUTPUT_DIR\" \\\n","    --captions_dir=\"$CAPTIONS_DIR\" \\\n","    --instance_prompt=\"$PT\" \\\n","    --seed=$Seed \\\n","    --resolution=$TexRes \\\n","    --mixed_precision=$precision \\\n","    --train_batch_size=1 \\\n","    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n","    --use_8bit_adam \\\n","    --learning_rate=$txlr \\\n","    --lr_scheduler=\"linear\" \\\n","    --lr_warmup_steps=0 \\\n","    --max_train_steps=$Training_Steps\n","\n","def train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n","    clear_output()\n","    if resuming==\"Yes\":\n","      print('\u001b[1;32mResuming Training...\u001b[0m')\n","    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n","    !accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py \\\n","    $extrnlcptn \\\n","    $ofstnse \\\n","    --image_captions_filename \\\n","    --train_only_unet \\\n","    --save_starting_step=$stpsv \\\n","    --save_n_steps=$stp \\\n","    --Session_dir=$SESSION_DIR \\\n","    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n","    --instance_data_dir=\"$INSTANCE_DIR\" \\\n","    --output_dir=\"$OUTPUT_DIR\" \\\n","    --captions_dir=\"$CAPTIONS_DIR\" \\\n","    --instance_prompt=\"$PT\" \\\n","    --seed=$Seed \\\n","    --resolution=$Res \\\n","    --mixed_precision=$precision \\\n","    --train_batch_size=1 \\\n","    --gradient_accumulation_steps=1 $GCUNET \\\n","    --use_8bit_adam \\\n","    --learning_rate=$untlr \\\n","    --lr_scheduler=\"linear\" \\\n","    --lr_warmup_steps=0 \\\n","    --max_train_steps=$Training_Steps\n","\n","\n","if Enable_text_encoder_training :\n","  print('\u001b[1;33mTraining the text encoder...\u001b[0m')\n","  if os.path.exists(OUTPUT_DIR+'/'+'text_encoder_trained'):\n","    %rm -r $OUTPUT_DIR\"/text_encoder_trained\"\n","  dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxt)\n","\n","if Enable_Text_Encoder_Concept_Training:\n","  if os.path.exists(CONCEPT_DIR):\n","    if os.listdir(CONCEPT_DIR)!=[]:\n","      clear_output()\n","      if resuming==\"Yes\":\n","        print('\u001b[1;32mResuming Training...\u001b[0m')\n","      print('\u001b[1;33mTraining the text encoder on the concept...\u001b[0m')\n","      dump_only_textenc(trnonltxt, MODELT_NAME, CONCEPT_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxtc)\n","    else:\n","      clear_output()\n","      if resuming==\"Yes\":\n","        print('\u001b[1;32mResuming Training...\u001b[0m')\n","      print('\u001b[1;31mNo concept images found, skipping concept training...')\n","      Text_Encoder_Concept_Training_Steps=0\n","      time.sleep(8)\n","  else:\n","      clear_output()\n","      if resuming==\"Yes\":\n","        print('\u001b[1;32mResuming Training...\u001b[0m')\n","      print('\u001b[1;31mNo concept images found, skipping concept training...')\n","      Text_Encoder_Concept_Training_Steps=0\n","      time.sleep(8)\n","\n","if UNet_Training_Steps!=0:\n","  train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps=UNet_Training_Steps)\n","\n","if UNet_Training_Steps==0 and Text_Encoder_Concept_Training_Steps==0 and Text_Encoder_Training_Steps==0 :\n","  print('\u001b[1;32mNothing to do')\n","else:\n","  if os.path.exists('/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n","    prc=\"--fp16\" if precision==\"fp16\" else \"\"\n","    !python /content/diffusers/scripts/convertosdv2.py $prc $OUTPUT_DIR $SESSION_DIR/$Session_Name\".ckpt\"\n","    clear_output()\n","    if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n","      clear_output()\n","      print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")\n","      if Disconnect_after_training :\n","        time.sleep(20)\n","        runtime.unassign()\n","    else:\n","      print(\"\u001b[1;31mSomething went wrong\")\n","  else:\n","    print(\"\u001b[1;31mSomething went wrong\")"]},{"cell_type":"markdown","metadata":{"id":"ehi1KKs-l-ZS"},"source":["# Test The Trained Model\n","\n","Here, we'll be able to launch a [Gradio app](https://gradio.app/) to test the model. After running this cell, click on the generated link to open a Gradio instance where you'll be able to prompt your newly generated checkpoint!\n","\n","Your cell will continue to show a \"running\" status while your gradio link is active. Make sure to regularly check the output of this cell, as Colab will not indicate \"execution completed\" and will instead have the Gradio link available and connected to this colab's GPU while the cell is in \"running\" status."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAZGngFcI8hq"},"outputs":[],"source":["import os\n","import time\n","import sys\n","import fileinput\n","from IPython.display import clear_output\n","from subprocess import getoutput\n","from IPython.utils import capture\n","from pyngrok import ngrok, conf\n","import base64\n","\n","blasphemy=base64.b64decode((\"d2VidWk=\")).decode('ascii')\n","\n","#@markdown # Optional Parameters (leave empty if running for the first time)\n","\n","Previous_Session=\"\" #@param{type: 'string'}\n","\n","#@markdown - Leave empty if you want to use the current trained model.\n","\n","Use_Custom_Path = False #@param {type:\"boolean\"}\n","\n","try:\n","  INSTANCE_NAME\n","  INSTANCET=INSTANCE_NAME\n","except:\n","  pass\n","#@markdown - if checked, an input box will ask the full path to a desired model.\n","\n","if Previous_Session!=\"\":\n","  INSTANCET=Previous_Session\n","  INSTANCET=INSTANCET.replace(\" \",\"_\")\n","\n","if Use_Custom_Path:\n","  try:\n","    INSTANCET\n","    del INSTANCET\n","  except:\n","    pass\n","\n","try:\n","  INSTANCET\n","  if Previous_Session!=\"\":\n","    path_to_trained_model='/content/gdrive/MyDrive/AdvancesInComputerVision_Pset7_Dreambooth/Sessions/'+Previous_Session+\"/\"+Previous_Session+'.ckpt'\n","  else:\n","    path_to_trained_model=SESSION_DIR+\"/\"+INSTANCET+'.ckpt'\n","except:\n","  print('\u001b[1;31mIt seems that you did not perform training during this session \u001b[1;32mor you chose to use a custom path,\\nprovide the full path to the model (including the name of the model):\\n')\n","  path_to_trained_model=input()\n","\n","while not os.path.exists(path_to_trained_model):\n","   print(\"\u001b[1;31mThe model doesn't exist on you Gdrive, use the file explorer to get the path : \")\n","   path_to_trained_model=input()\n","\n","fgitclone = \"git clone --depth 1\"\n","\n","with capture.capture_output() as cap:\n","    if not os.path.exists('/content/gdrive/MyDrive'):\n","      !mkdir -p /content/gdrive/MyDrive\n","\n","if not os.path.exists('/content/gdrive/MyDrive/sd/stablediffusion'):\n","    !wget -q -O /content/sd_rep.tar.zst https://huggingface.co/TheLastBen/dependencies/resolve/main/sd_rep.tar.zst\n","    !tar -C  /content/gdrive/MyDrive --zstd -xf /content/sd_rep.tar.zst\n","    !rm /content/sd_rep.tar.zst\n","    clear_output()\n","\n","with capture.capture_output() as cap:\n","  %cd /content/gdrive/MyDrive/sd\n","  !git clone -q --branch master https://github.com/AUTOMATIC1111/stable-diffusion-$blasphemy\n","  %cd stable-diffusion-$blasphemy\n","  !mkdir cache\n","  !sed -i 's@~/.cache@/content/gdrive/MyDrive/sd/stable-diffusion-{blasphemy}/cache@' /usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py\n","\n","  clear_output()\n","  !git reset --hard\n","  time.sleep(1)\n","  !rm webui.sh\n","  !git pull\n","  !git fetch --unshallow\n","  !git checkout a9eab236d7e8afa4d6205127904a385b2c43bb24\n","\n","with capture.capture_output() as cap:\n","  if not os.path.exists('/tools/node/bin/lt'):\n","    !npm install -g localtunnel\n","\n","auth=\"\"\n","\n","\n","with capture.capture_output() as cap:\n","  %cd modules\n","  !wget -q -O paths.py https://github.com/TheLastBen/fast-stable-diffusion/raw/5632d2ef7fffd940976538d270854ec4faf26855/AUTOMATIC1111_files/paths.py\n","  !wget -q -O extras.py https://github.com/AUTOMATIC1111/stable-diffusion-$blasphemy/raw/a9eab236d7e8afa4d6205127904a385b2c43bb24/modules/extras.py\n","  !wget -q -O sd_models.py https://github.com/AUTOMATIC1111/stable-diffusion-$blasphemy/raw/a9eab236d7e8afa4d6205127904a385b2c43bb24/modules/sd_models.py\n","  !wget -q -O /usr/local/lib/python3.9/dist-packages/gradio/blocks.py https://github.com/TheLastBen/fast-stable-diffusion/raw/7ff88eaa1fb4997bacd9845bd487f9a14335d625/AUTOMATIC1111_files/blocks.py\n","  %cd /content/gdrive/MyDrive/sd/stable-diffusion-$blasphemy/\n","\n","  !sed -i \"s@os.path.splitext(checkpoint_file)@os.path.splitext(checkpoint_file); map_location='cuda'@\" /content/gdrive/MyDrive/sd/stable-diffusion-$blasphemy/modules/sd_models.py\n","  !sed -i 's@ui.create_ui().*@ui.create_ui();shared.demo.queue(concurrency_count=999999,status_update_rate=0.1)@' /content/gdrive/MyDrive/sd/stable-diffusion-$blasphemy/webui.py\n","  !sed -i \"s@map_location='cpu'@map_location='cuda'@\" /content/gdrive/MyDrive/sd/stable-diffusion-$blasphemy/modules/extras.py\n","  !sed -i 's@print(\\\"No module.*@@' /content/gdrive/MyDrive/sd/stablediffusion/ldm/modules/diffusionmodules/model.py\n","  !sed -i 's@\\\"quicksettings\\\": OptionInfo(.*@\"quicksettings\": OptionInfo(\"sd_model_checkpoint,  sd_vae, CLIP_stop_at_last_layers, inpainting_mask_weight, initial_noise_multiplier\", \"Quicksettings list\"),@' /content/gdrive/MyDrive/sd/stable-diffusion-$blasphemy/modules/shared.py\n","\n","share='--share'\n","\n","configf=\"--api --disable-safe-unpickle --enable-insecure-extension-access --no-half-vae --opt-sdp-attention --no-download-sd-model --disable-console-progressbars\"\n","\n","clear_output()\n","\n","if os.path.isfile(path_to_trained_model):\n","  !python /content/gdrive/MyDrive/sd/stable-diffusion-$blasphemy/webui.py $share --ckpt \"$path_to_trained_model\" $auth $configf\n","else:\n","  !python /content/gdrive/MyDrive/sd/stable-diffusion-$blasphemy/webui.py $share --ckpt-dir \"$path_to_trained_model\" $auth $configf"]},{"cell_type":"markdown","source":["# Your objective for this problem: compare any two dreambooth models and report results.\n","\n","Try at least 2 dreambooth models and compare the results. You have complete freedom to test any two dreambooth models, including:\n","- Checkpoint 1 trained for X steps vs Checkpoint 2 resumed from Checkpoint 1 and trained for another Y steps\n","- Checkpoint 1 with an object as concept vs Checkpoint 2 trained with a style as concept\n","- Checkpoint 1 with one set of training parameters vs Checkpoint 2 trained with another set\n","- Any other meaningful difference that you'd like to explore!"],"metadata":{"id":"I21NiNZ_86ql"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}